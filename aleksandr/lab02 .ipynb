{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5f7016a588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc, desc, lower #lit, #array\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #регулярные выражения\n",
    "import tqdm # прогресс-бар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_films = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.repartition(6)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+----+--------------------+--------+\n",
      "|                 cat|                desc|  id|lang|                name|provider|\n",
      "+--------------------+--------------------+----+----+--------------------+--------+\n",
      "| 6/economics_finance|In this course, y...| 472|  en|The Power of Micr...|Coursera|\n",
      "|                    | BECOME A PR PRO ...|7479|  en|Public Relations ...|   Udemy|\n",
      "|                    | Learn SAP ABAP P...|4917|  en|SAP ABAP Programm...|   Udemy|\n",
      "|11/law|14/social_...|An introduction t...| 151|  en|  Constitutional Law|Coursera|\n",
      "|3/business_manage...|\n",
      "Committing to ta...|7413|  zh|Introduction to C...|   Udemy|\n",
      "+--------------------+--------------------+----+----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_desc_lang = df.select([c for c in df.columns if c in ['id','desc', 'lang']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stop_words_ru = StopWordsRemover.loadDefaultStopWords(\"russian\")\n",
    "stop_words_es = StopWordsRemover.loadDefaultStopWords(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data, column):\n",
    "    data=data.withColumn(column, lower(col('desc')))\n",
    "    data = data.withColumn(column, F.regexp_replace('desc', '[!@\"“’«»#$%&\\'()*+,—/:;<=>?^_`{|}~\\[\\]]', ''))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_desc_lang_clean = clean_text(id_desc_lang, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'ru').select('id', 'desc')\n",
    "en_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'en').select('id', 'desc')\n",
    "es_data = id_desc_lang_clean.filter(id_desc_lang_clean.lang == 'es').select('id', 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24553"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтобы понять ноебходимо повторить bagofwords tf-idf \n",
    "list_ru = ru_data.select('desc').collect()\n",
    "list_en = en_data.select('desc').collect()\n",
    "list_es = es_data.select('desc').collect()\n",
    "lst_full_language = [list_ru, list_en, list_es]\n",
    "#mvv_array = [int(row.mvv) for row in mvv_list.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20335\n",
      "347337\n",
      "57222\n"
     ]
    }
   ],
   "source": [
    "#расчет количество фичей в зависимости от уникальных слов в дата-сете\n",
    "for lst in lst_full_language:\n",
    "    set_lang = set()\n",
    "    for i in lst:\n",
    "        for j in i[0].split():\n",
    "            set_lang.add(j)\n",
    "    print(len(set_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиваем текст на токены\n",
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_en)\n",
    "\n",
    "hasher = HashingTF(numFeatures=347337, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_en = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_en = pipeline_en.fit(en_data)\n",
    "tr_df_en = pipeline_model_en.transform(en_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|            features|\n",
      "+----+--------------------+\n",
      "|8133|(347337,[1927,399...|\n",
      "|1629|(347337,[3082,967...|\n",
      "|8851|(347337,[1684,244...|\n",
      "|3937|(347337,[10092,10...|\n",
      "|6182|(347337,[914,2159...|\n",
      "|6026|(347337,[449,515,...|\n",
      "|6742|(347337,[2479,100...|\n",
      "|6898|(347337,[1637,719...|\n",
      "|5801|(347337,[2479,276...|\n",
      "|1858|(347337,[2626,623...|\n",
      "|7257|(347337,[1415,244...|\n",
      "|7196|(347337,[1720,267...|\n",
      "|4063|(347337,[475,1087...|\n",
      "|5828|(347337,[3565,719...|\n",
      "| 263|(347337,[475,2368...|\n",
      "|6439|(347337,[3293,756...|\n",
      "|2069|(347337,[1131,182...|\n",
      "|6696|(347337,[1495,356...|\n",
      "|3382|(347337,[23303,23...|\n",
      "|3256|(347337,[2899,598...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_df_en.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_ru)\n",
    "\n",
    "hasher = HashingTF(numFeatures=20335, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_ru = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_ru = pipeline_ru.fit(ru_data)\n",
    "tr_df_ru = pipeline_model_ru.transform(ru_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "\n",
    "swr_en = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered_en\", stopWords=stop_words_es)\n",
    "\n",
    "hasher = HashingTF(numFeatures=57222, binary=False, inputCol=swr_en.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "\n",
    "pipeline_es = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr_en,\n",
    "    hasher,\n",
    "    idf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_es = pipeline_es.fit(es_data)\n",
    "tr_df_es = pipeline_model_es.transform(es_data).select('id', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v,u):\n",
    "    return float(v.dot(u) / (v.norm(2) * u.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатывается рекомендаций: 1\n",
      "Обрабатывается рекомендаций: 2\n",
      "Обрабатывается рекомендаций: 3\n",
      "Обрабатывается рекомендаций: 4\n",
      "Обрабатывается рекомендаций: 5\n",
      "Обрабатывается рекомендаций: 6\n"
     ]
    }
   ],
   "source": [
    "film_recomendation = {}\n",
    "count = 0\n",
    "for i in my_films:\n",
    "    count += 1\n",
    "    print(\"Обрабатывается рекомендаций:\", count)\n",
    "    if i[1] == 'ru':\n",
    "        df_lang = tr_df_ru\n",
    "    elif i[1] == 'en':\n",
    "        df_lang = tr_df_en\n",
    "    elif i[1] == 'es':\n",
    "        df_lang = tr_df_es\n",
    "    lst_id = []\n",
    "    lst_cosine = []\n",
    "    f_vec = df_lang.filter(df_lang.id == i[0]).collect()[0]['features']\n",
    "    for itertator in df_lang.filter(df_lang.id != i[0]).collect():\n",
    "        lst_id.append(itertator['id'])\n",
    "        lst_cosine.append(cos_sim(itertator['features'], f_vec))\n",
    "    res = sqlContext.createDataFrame(zip(lst_id, lst_cosine), schema=['id', 'cos'])\n",
    "    res = res.repartition(6)\n",
    "    res = res.dropna()\n",
    "    res = res.sort(\"cos\", ascending=False).collect()[0:10]\n",
    "    lst = []\n",
    "    for j in res:\n",
    "        lst.append(j[0])\n",
    "    film_recomendation[i[0]] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [13702, 'ru', 'Математическая экономика']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [13665, 13782, 15909, 25782, 14760, 13348, 19270, 17499, 25071, 7153],\n",
       " 21617: [21609, 21608, 21616, 21492, 21703, 21675, 21506, 21624, 21623, 21630],\n",
       " 16627: [11431, 12247, 5687, 17964, 12660, 16694, 5558, 9563, 10738, 13529],\n",
       " 11556: [16488, 13461, 22710, 468, 10447, 23357, 11523, 19330, 12679, 9289],\n",
       " 16704: [1228, 1327, 20362, 1215, 13696, 1365, 26980, 1236, 8186, 875],\n",
       " 13702: [864, 21079, 1052, 8123, 1396, 1041, 1033, 13057, 1217, 8313]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_recomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as outfile:\n",
    "    json.dump(film_recomendation, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
